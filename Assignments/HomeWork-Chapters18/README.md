# HW to Chapter 18 “Neural Style Transfer”

# Non-programming Assignment

## 1. What is face verification and how does it work?

**Face verification** is the process of determining whether two given images belong to the same person. It is a binary classification task, often used in applications like authentication systems (e.g., unlocking a phone with a face scan).

### How it works:
1. **Feature Extraction**: A deep neural network extracts features (e.g., embeddings) from both images.
2. **Comparison**: The embeddings are compared using a similarity metric, such as cosine similarity or Euclidean distance.
3. **Decision**: A threshold is applied to the similarity score. If the score is above the threshold, the images are classified as the same person; otherwise, they are not.

---

## 2. Describe the difference between face verification and face recognition

### Face Verification:
- **Task**: Determines if two images are of the same person.
- **Input**: Two images to compare.
- **Output**: Binary decision (same person or not).
- **Use Case**: Authentication systems (e.g., unlocking devices).

### Face Recognition:
- **Task**: Identifies a person in an image by matching it to a database of known faces.
- **Input**: A single image.
- **Output**: The identity of the person (or "unknown" if no match).
- **Use Case**: Surveillance systems or photo tagging.

---

## 3. How do you measure similarity of images?

Image similarity is measured using feature embeddings generated by neural networks. Common metrics include:
1. **Cosine Similarity**: Measures the cosine of the angle between two vectors.
2. **Euclidean Distance**: Measures the straight-line distance between two points in the embedding space.
3. **Manhattan Distance**: Measures the sum of absolute differences between embedding coordinates.
4. **Structural Similarity Index (SSIM)**: Compares images based on luminance, contrast, and structure.

The choice of metric depends on the application and the scale of the embeddings.

---

## 4. Describe Siamese networks

**Siamese networks** are neural network architectures designed to learn similarity between two inputs. They consist of two identical subnetworks that share weights, enabling them to process two inputs symmetrically.

### How it works:
1. **Feature Extraction**: Each input is passed through one of the subnetworks to extract embeddings.
2. **Comparison**: The embeddings are compared using a similarity metric (e.g., Euclidean distance or cosine similarity).
3. **Output**: The network outputs a score indicating the similarity of the two inputs.

Siamese networks are widely used for tasks like face verification, signature verification, and one-shot learning.

---

## 5. What is triplet loss and why is it needed?

**Triplet loss** is a loss function used to train models for tasks like face verification and recognition by ensuring that embeddings of similar images are closer together while embeddings of dissimilar images are farther apart.

### Why it’s needed:
To improve the discriminative ability of embeddings, triplet loss optimizes the model to satisfy the condition:
\[ \text{distance(anchor, positive)} + \alpha < \text{distance(anchor, negative)} \]
Where:
- **Anchor**: A reference image.
- **Positive**: An image of the same person as the anchor.
- **Negative**: An image of a different person.
- **\(\alpha\)**: A margin to ensure separation.

By minimizing triplet loss, the model learns robust embeddings that make similar images closer and dissimilar images farther apart in the embedding space.

---

## 6. What is neural style transfer (NST) and how does it work?

**Neural Style Transfer (NST)** is a technique that applies the artistic style of one image (style image) to another image (content image) while preserving the original content.

### How it works:
1. **Feature Extraction**: A pretrained convolutional neural network (e.g., VGG19) extracts features from both the content and style images.
2. **Style Representation**: Style features are captured using Gram matrices of the feature maps.
3. **Optimization**: A new image (generated image) is initialized (often random noise or the content image itself) and iteratively updated to minimize a cost function combining content and style differences:
   - **Content Loss**: Ensures the generated image preserves content features.
   - **Style Loss**: Ensures the generated image matches the style features.
   - **Total Variation Loss** (optional): Promotes smoothness in the generated image.

The result is an image that combines the content of one image with the style of another.

---

## 7. Describe style cost function

The **style cost function** measures how closely the style of the generated image matches the style of the style image. It uses the Gram matrix, which captures the correlations between feature maps at a given layer of a neural network.

### Formula:
\[
L_{style} = \sum_{l} w_l \cdot || G^l_{generated} - G^l_{style} ||^2
\]
Where:
- **\(l\)**: Layer index.
- **\(G^l_{generated}\)**: Gram matrix of the generated image at layer \(l\).
- **\(G^l_{style}\)**: Gram matrix of the style image at layer \(l\).
- **\(w_l\)**: Weight for layer \(l\).

The style cost encourages the generated image to reproduce the textures, patterns, and colors of the style image.

---